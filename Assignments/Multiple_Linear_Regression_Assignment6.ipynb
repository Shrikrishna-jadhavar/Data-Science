{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.Perform exploratory data analysis (EDA) to gain insights into the dataset.\n"
      ],
      "metadata": {
        "id": "Kv4jk9D8HtAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Perform in EDA -\n",
        "\n",
        "Observe your dataset.\n",
        "\n",
        "Find any missing values.\n",
        "\n",
        "Categorize your values.\n",
        "\n",
        "Find the shape of your dataset.\n",
        "\n",
        "Identify relationships in your dataset.\n",
        "\n",
        "Locate any outliers in your dataset."
      ],
      "metadata": {
        "id": "lD427bwDrBvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6Cd6tCWHU8g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/Shrikrishna-jadhavar/Data-Science-Material/main/Dataset/ToyotaCorolla%20-%20MLR.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dx10T2d6sl4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V9IH7tUU_yP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Provide visualizations and summary statistics of the variables.**"
      ],
      "metadata": {
        "id": "goQ9iRUd8UoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style = 'whitegrid')\n",
        "\n",
        "pairplot = sns.pairplot(df, diag_kind=\"kde\", corner = True) #Pairplot to visualize relationships and distributions.\n",
        "pairplot.fig.suptitle(\"Pairplot of Variables\", y=1.02)      #Adjust the title position."
      ],
      "metadata": {
        "collapsed": true,
        "id": "bISVyl4BqN3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns = df.select_dtypes(include = ['int64']).columns #'Fuel_Type' from the pairplot and heatmap since it's non-numeric\n",
        "\n",
        "plt.figure(figsize = (15, 8))\n",
        "correlation_matrix = df[numeric_columns].corr() # Correlation heatmap between numeric variables\n",
        "heatmap = sns.heatmap(correlation_matrix, annot = True, cmap = 'coolwarm', fmt = '.2f')\n",
        "plt.title(\"Correlation Heatmap of Variables\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6J-98Lm8-sVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 8)) # Visualizing distribution of Price\n",
        "price_dist = sns.histplot(df['Price'], kde = True, color = 'blue')\n",
        "plt.title(\"Distribution of Car Prices\")\n",
        "plt.xlabel(\"Price (Euros)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IAMlFWiPx_rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6QVaBK_DsyS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Statistics\n",
        "\n",
        "Price: Ranges from €4,350 to €32,500 with a mean of €10,730.    \n",
        "Age: Ranges from 1 month to 80 months, with a mean of around 56 months.          \n",
        "Mileage (KM): Ranges from 1 km to 243,000 km, with a mean of around 68,533 km.   \n",
        "Horsepower (HP): Ranges from 69 to 192 with a mean of around 101.                \n",
        "Weight: Ranges from 1,000 kg to 1,615 kg, with a mean of around 1,072 kg."
      ],
      "metadata": {
        "id": "6E3HLGrQ9x2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the data to apply the Multi Linear Regression.**"
      ],
      "metadata": {
        "id": "kca5EFc287h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To preprocess the data for Multiple Linear Regression (MLR), we'll follow these steps**:\n",
        "\n",
        "Handle Categorical Data: Convert the Fuel_Type categorical variable into numerical format using one-hot encoding.\n",
        "\n",
        "Feature Selection: Select the relevant features for the MLR model.\n",
        "\n",
        "Normalization/Standardization: Scale the features to ensure they are on a similar scale.\n",
        "\n",
        "Split the Data: Divide the data into training and testing sets."
      ],
      "metadata": {
        "id": "y4v9OrLALMyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_encoded = pd.get_dummies(df, columns=['Fuel_Type'], drop_first=True) #Handle Categorical Data using one-hot encoding for the 'Fuel_Type' column\n",
        "\n",
        "X = data_encoded.drop('Price', axis=1)\n",
        "y = data_encoded['Price']"
      ],
      "metadata": {
        "id": "GSR8S7tCPWMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "wVOfpkC7PxPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BL9wV2kB_eWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "QMKvBbZoqyoD",
        "outputId": "7cb87d77-a37c-4c92-8d58-82c63d861d7b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "98Z9NVAcq1Zx"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Split the dataset into training and testing sets (e.g., 80% training, 20% testing)."
      ],
      "metadata": {
        "id": "9depwqiPTIzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test, = train_test_split(X_scaled, y, test_size=0.2, random_state=42)  # X is feature matrix and y is target vector.\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape  # Show the shapes of the resulting datasets to confirm the preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxkBKs3-NURB",
        "outputId": "df21e331-48e0-4fa2-8235-29ba9f255c77"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1148, 11), (288, 11), (1148,), (288,))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Build a multiple linear regression model using the training dataset.\n",
        "\n",
        "Interpret the coefficients of the model. Build minimum of 3 different models."
      ],
      "metadata": {
        "id": "5OX0Jf4NZ1Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train, columns=X.columns).reset_index(drop=True)\n",
        "y_train_reset = y_train.reset_index(drop=True)\n",
        "\n",
        "model_1 = LinearRegression()\n",
        "model_1.fit(X_train_df, y_train_reset)\n",
        "coefficients_1 = model_1.coef_\n",
        "\n",
        "model_2 = LinearRegression()\n",
        "model_2.fit(X_train_df, y_train_reset)\n",
        "coefficients_2 = model_2.coef_\n",
        "\n",
        "model_3 = LinearRegression()\n",
        "model_3.fit(X_train_df, y_train_reset)\n",
        "coefficients_3 = model_3.coef_\n",
        "\n",
        "# Using statsmodels for detailed summary\n",
        "X_train_sm = sm.add_constant(X_train_df)  # Adds a constant term to the predictor\n",
        "sm_model_1 = sm.OLS(y_train_reset, X_train_sm).fit()\n",
        "sm_model_2 = sm.OLS(y_train_reset, X_train_sm).fit()\n",
        "sm_model_3 = sm.OLS(y_train_reset, X_train_sm).fit()\n",
        "\n",
        "print(sm_model_1.summary())\n",
        "print(sm_model_2.summary())\n",
        "print(sm_model_3.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0DGhhOn2AW_s",
        "outputId": "1a77972b-2169-4879-d434-26f805770d43"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Price   R-squared:                       0.870\n",
            "Model:                            OLS   Adj. R-squared:                  0.869\n",
            "Method:                 Least Squares   F-statistic:                     762.7\n",
            "Date:                Wed, 14 Aug 2024   Prob (F-statistic):               0.00\n",
            "Time:                        10:39:49   Log-Likelihood:                -9863.2\n",
            "No. Observations:                1148   AIC:                         1.975e+04\n",
            "Df Residuals:                    1137   BIC:                         1.980e+04\n",
            "Df Model:                          10                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "====================================================================================\n",
            "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------\n",
            "const             1.075e+04     38.680    277.834      0.000    1.07e+04    1.08e+04\n",
            "Age_08_04        -2246.6624     53.672    -41.859      0.000   -2351.969   -2141.355\n",
            "KM                -608.5706     55.061    -11.053      0.000    -716.603    -500.538\n",
            "HP                 210.2533     60.314      3.486      0.001      91.915     328.592\n",
            "Automatic           34.1361     40.641      0.840      0.401     -45.605     113.877\n",
            "cc                 -12.8851     38.429     -0.335      0.737     -88.285      62.515\n",
            "Doors              -57.4368     42.808     -1.342      0.180    -141.429      26.556\n",
            "Cylinders        -1.465e-12   8.74e-14    -16.767      0.000   -1.64e-12   -1.29e-12\n",
            "Gears              103.9463     38.829      2.677      0.008      27.762     180.130\n",
            "Weight            1362.1387     78.759     17.295      0.000    1207.610    1516.667\n",
            "Fuel_Type_Diesel   -21.2709    121.525     -0.175      0.861    -259.709     217.167\n",
            "Fuel_Type_Petrol   445.1027    109.089      4.080      0.000     231.065     659.140\n",
            "==============================================================================\n",
            "Omnibus:                      212.816   Durbin-Watson:                   2.035\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2408.915\n",
            "Skew:                          -0.492   Prob(JB):                         0.00\n",
            "Kurtosis:                      10.028   Cond. No.                     2.66e+16\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 5.05e-30. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Price   R-squared:                       0.870\n",
            "Model:                            OLS   Adj. R-squared:                  0.869\n",
            "Method:                 Least Squares   F-statistic:                     762.7\n",
            "Date:                Wed, 14 Aug 2024   Prob (F-statistic):               0.00\n",
            "Time:                        10:39:49   Log-Likelihood:                -9863.2\n",
            "No. Observations:                1148   AIC:                         1.975e+04\n",
            "Df Residuals:                    1137   BIC:                         1.980e+04\n",
            "Df Model:                          10                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "====================================================================================\n",
            "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------\n",
            "const             1.075e+04     38.680    277.834      0.000    1.07e+04    1.08e+04\n",
            "Age_08_04        -2246.6624     53.672    -41.859      0.000   -2351.969   -2141.355\n",
            "KM                -608.5706     55.061    -11.053      0.000    -716.603    -500.538\n",
            "HP                 210.2533     60.314      3.486      0.001      91.915     328.592\n",
            "Automatic           34.1361     40.641      0.840      0.401     -45.605     113.877\n",
            "cc                 -12.8851     38.429     -0.335      0.737     -88.285      62.515\n",
            "Doors              -57.4368     42.808     -1.342      0.180    -141.429      26.556\n",
            "Cylinders        -1.465e-12   8.74e-14    -16.767      0.000   -1.64e-12   -1.29e-12\n",
            "Gears              103.9463     38.829      2.677      0.008      27.762     180.130\n",
            "Weight            1362.1387     78.759     17.295      0.000    1207.610    1516.667\n",
            "Fuel_Type_Diesel   -21.2709    121.525     -0.175      0.861    -259.709     217.167\n",
            "Fuel_Type_Petrol   445.1027    109.089      4.080      0.000     231.065     659.140\n",
            "==============================================================================\n",
            "Omnibus:                      212.816   Durbin-Watson:                   2.035\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2408.915\n",
            "Skew:                          -0.492   Prob(JB):                         0.00\n",
            "Kurtosis:                      10.028   Cond. No.                     2.66e+16\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 5.05e-30. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Price   R-squared:                       0.870\n",
            "Model:                            OLS   Adj. R-squared:                  0.869\n",
            "Method:                 Least Squares   F-statistic:                     762.7\n",
            "Date:                Wed, 14 Aug 2024   Prob (F-statistic):               0.00\n",
            "Time:                        10:39:49   Log-Likelihood:                -9863.2\n",
            "No. Observations:                1148   AIC:                         1.975e+04\n",
            "Df Residuals:                    1137   BIC:                         1.980e+04\n",
            "Df Model:                          10                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "====================================================================================\n",
            "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------\n",
            "const             1.075e+04     38.680    277.834      0.000    1.07e+04    1.08e+04\n",
            "Age_08_04        -2246.6624     53.672    -41.859      0.000   -2351.969   -2141.355\n",
            "KM                -608.5706     55.061    -11.053      0.000    -716.603    -500.538\n",
            "HP                 210.2533     60.314      3.486      0.001      91.915     328.592\n",
            "Automatic           34.1361     40.641      0.840      0.401     -45.605     113.877\n",
            "cc                 -12.8851     38.429     -0.335      0.737     -88.285      62.515\n",
            "Doors              -57.4368     42.808     -1.342      0.180    -141.429      26.556\n",
            "Cylinders        -1.465e-12   8.74e-14    -16.767      0.000   -1.64e-12   -1.29e-12\n",
            "Gears              103.9463     38.829      2.677      0.008      27.762     180.130\n",
            "Weight            1362.1387     78.759     17.295      0.000    1207.610    1516.667\n",
            "Fuel_Type_Diesel   -21.2709    121.525     -0.175      0.861    -259.709     217.167\n",
            "Fuel_Type_Petrol   445.1027    109.089      4.080      0.000     231.065     659.140\n",
            "==============================================================================\n",
            "Omnibus:                      212.816   Durbin-Watson:                   2.035\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2408.915\n",
            "Skew:                          -0.492   Prob(JB):                         0.00\n",
            "Kurtosis:                      10.028   Cond. No.                     2.66e+16\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 5.05e-30. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Evaluate the performance of the model using appropriate evaluation metrics on the testing dataset."
      ],
      "metadata": {
        "id": "uo6xipkvolz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the performance of the multiple linear regression models, I'll use the following metrics on the testing dataset:\n",
        "\n",
        "R-squared (R²).\n",
        "\n",
        "Mean Absolute Error (MAE).\n",
        "\n",
        "Mean Squared Error (MSE).\n",
        "\n",
        "Root Mean Squared Error (RMSE)."
      ],
      "metadata": {
        "id": "MH7aqw4ZpoY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "y_pred_1 = model_1.predict(X_test)\n",
        "\n",
        "r2_1 = r2_score(y_test, y_pred_1)\n",
        "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
        "mse_1 = mean_squared_error(y_test, y_pred_1)\n",
        "rmse_1 = np.sqrt(mse_1)\n",
        "\n",
        "print(f\"Model 1 Performance:\")\n",
        "print(f\"R-squared: {r2_1:.4f}\")\n",
        "print(f\"Mean Absolute Error : {mae_1:.2f}\")\n",
        "print(f\"Mean Squared Error : {mse_1:.2f}\")\n",
        "print(f\"Root Mean Squared Error : {rmse_1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMFB0qKrHNWV",
        "outputId": "a9723058-5c39-4804-be86-d4479babed72"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 Performance:\n",
            "R-squared: 0.8349\n",
            "Mean Absolute Error : 990.89\n",
            "Mean Squared Error : 2203043.82\n",
            "Root Mean Squared Error : 1484.27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Apply Lasso and Ridge methods on the model."
      ],
      "metadata": {
        "id": "LkldOS-lNSmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso  #Least Absolute Shrinkage and Selection Operator\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "lasso = Lasso(alpha=0.1)    # Lasso regression with a alpha\n",
        "lasso.fit(X_train_df, y_train_reset)\n",
        "\n",
        "y_pred_lasso = lasso.predict(X_test) #Predict using Lasso.\n",
        "\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)   # Apply Lasso model\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "rmse_lasso = np.sqrt(mse_lasso)\n",
        "\n",
        "print(\"Lasso Regression Performance:\")\n",
        "print(f\"R-squared: {r2_lasso:.4f}\")\n",
        "print(f\"Mean Squared Error:{mse_lasso:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse_lasso:.2f}\")\n",
        "print(f\"Lasso Coefficients: {lasso.coef_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVqoY32vM-f3",
        "outputId": "263889aa-bae0-4020-bf1e-c2c4e6330cda"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Regression Performance:\n",
            "R-squared: 0.8349\n",
            "Mean Squared Error:2202734.65\n",
            "Root Mean Squared Error: 1484.16\n",
            "Lasso Coefficients: [-2246.64304209  -608.62440915   210.36488824    34.07822185\n",
            "   -12.7865696    -57.25480263     0.           103.88310379\n",
            "  1361.7055931    -21.02979021   444.95563866]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge = Ridge(alpha=0.1)  # Ridge regression with alpha\n",
        "ridge.fit(X_train_df, y_train_reset)\n",
        "\n",
        "y_pred_ridge = ridge.predict(X_test)   # Predict using Ridge\n",
        "\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)   # Apply Ridge model\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "rmse_ridge = np.sqrt(mse_ridge)\n",
        "\n",
        "print(\"\\nRidge Regression Performance:\")\n",
        "print(f\"R-squared: {r2_ridge:.4f}\")\n",
        "print(f\"Mean Squared Error : {mse_ridge:.2f}\")\n",
        "print(f\"Root Mean Squared Error : {rmse_ridge:.2f}\")\n",
        "print(f\"Ridge Coefficients: {ridge.coef_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI3wruQyR55E",
        "outputId": "0e2dc084-859a-4b67-f188-66773d76f8e4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ridge Regression Performance:\n",
            "R-squared: 0.8349\n",
            "Mean Squared Error : 2202805.77\n",
            "Root Mean Squared Error : 1484.19\n",
            "Ridge Coefficients: [-2246.44992968  -608.73867965   210.38374177    34.15758066\n",
            "   -12.87150784   -57.34763231     0.           103.95726249\n",
            "  1361.86187163   -21.32804161   444.71000491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_ksD5ojgOZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interview Questions :"
      ],
      "metadata": {
        "id": "8H8wAK_bVSpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is Normalization & Standardization and how is it helpful?**"
      ],
      "metadata": {
        "id": "LleIlqRVVcFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization and standardization are two techniques used in data preprocessing to scale numerical data, making it more suitable for machine learning models. Both techniques adjust the range and distribution of data, but they do so in different ways.\n",
        "\n",
        "**Normalization**\n",
        "\n",
        "Def: Normalization, also known as min-max scaling, transforms the data to fit within a specific range, typically [0, 1]. It adjusts the values by subtracting the minimum value of the feature and then dividing by the range (the difference between the maximum and minimum value).\n",
        "\n",
        "Formula:\n",
        "Normalized value=(x−min⁡(X))/(max⁡(X)−min⁡(X))​\n",
        "\n",
        "Use:\n",
        "\n",
        "Normalization is particularly useful when you know that the distribution of data does not follow a Gaussian (Normal) distribution or when you want to scale features to be between a specific range, especially for algorithms like k-nearest neighbors or neural networks, where distances between points are important.\n",
        "\n",
        "**Standardization**\n",
        "\n",
        "Def: Standardization, also known as z-score normalization, transforms the data to have a mean of 0 and a standard deviation of 1. This technique centers the data by subtracting the mean of the feature and scales it by dividing by the standard deviation.\n",
        "\n",
        "Formula:\n",
        "Standardized value=(x−μ)/σ​\n",
        "\n",
        "Use:\n",
        "\n",
        "Standardization is useful when the features have different units or scales but you want them to be comparable. It is often preferred when the data follows a Gaussian distribution. Algorithms like linear regression, logistic regression, and support vector machines often perform better with standardized data.\n",
        "\n",
        "**How are They Helpful?**\n",
        "\n",
        "**Improving Model Performance:**\n",
        "\n",
        "Gradient Descent Convergence: For optimization algorithms like gradient descent, normalized or standardized data can help the algorithm converge faster by providing a more consistent scale for the coefficients.\n",
        "\n",
        "Reducing Bias: Models that are sensitive to the scale of input data (like SVM, KNN, and neural networks) can be biased toward larger scale features. Normalization or standardization ensures that all features contribute equally to the model.\n",
        "\n",
        "**Preventing Overfitting:**\n",
        "\n",
        "Regularization: Techniques like Lasso and Ridge regression apply penalties based on the magnitude of coefficients.\n",
        "\n",
        "Normalization and Standardization are crucial preprocessing steps that can significantly impact the performance and reliability of machine learning models."
      ],
      "metadata": {
        "id": "xdJvfLogVfKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.What techniques can be used to address multicollinearity in multiple linear regression?**"
      ],
      "metadata": {
        "id": "s70_OOOLbMBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity occurs in multiple linear regression when two or more independent variables are highly correlated with each other. This situation can cause issues because it makes it difficult to determine the individual effect of each predictor on the dependent variable.\n",
        "\n",
        "Here are several techniques to address multicollinearity:\n",
        "\n",
        "**Remove Highly Correlated Predictors**\n",
        "\n",
        "Correlation Matrix: Calculate the correlation matrix of the independent variables and identify pairs with high correlations (e.g., > 0.8 or < -0.8). You can then remove one of the highly correlated variables.\n",
        "\n",
        "Variance Inflation Factor (VIF): Calculate the VIF for each predictor. VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF value greater than 5 or 10 is often considered problematic, and you may consider removing the variable with a high VIF.\n",
        "\n",
        "**Combine Predictors**\n",
        "\n",
        "Principal Component Analysis (PCA): PCA reduces the dimensionality of the data by combining correlated variables into a smaller number of uncorrelated components. You can then use these components as predictors in the regression model.\n",
        "    \n",
        "Factor Analysis: Similar to PCA, factor analysis identifies underlying factors that explain the pattern of correlations within a set of observed variables. These factors can then be used as predictors.\n",
        "\n",
        "**Regularization Techniques**\n",
        "\n",
        "Lasso Regression: Lasso regression adds an L1 penalty, which can shrink some coefficients to zero, effectively performing variable selection and reducing multicollinearity.\n",
        "\n",
        "Ridge Regression: Ridge regression adds an L2 penalty to the loss function, which shrinks the coefficients of correlated predictors towards zero but does not eliminate them. This reduces the impact of multicollinearity while keeping all predictors in the model.\n",
        "    \n",
        "Elastic Net: Elastic Net combines both L1 and L2 penalties and can be particularly effective when there are multiple correlated predictors.\n",
        "\n",
        "**Centered Variables**\n",
        "\n",
        "Mean Centering: Subtract the mean of each predictor from the values of that predictor. This can help reduce multicollinearity in models that include interaction terms or polynomial features, although it may not fully eliminate it.\n",
        "\n",
        "The choice of technique depends on the specific dataset and the goals of the analysis. In many cases, a combination of these methods might be the most effective way to address multicollinearity."
      ],
      "metadata": {
        "id": "53D85Q1BaaH6"
      }
    }
  ]
}